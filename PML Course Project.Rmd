---
title: "Practical Machine Learning Course Project"
author: "George Ng"
date: "25/01/2017"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Practical Machine Learning Project : Prediction Assignment Writeup

## Executive Summary

This R Markdown shows both the R code and results of the Practical Machine Learning Course Project in a HTML file processed by knitr. Devices such as Jawbone Up, Nike FuelBand, and Fitbit enable the collection of a large amount of gym activity data. The data is collected from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. The outcome variable has five classes and the total number of predictors are 159. The machine learning algorithm described here is applied to the 20 test cases in the test data for a seperate Course Project Prediction quiz and the predictions are submitted for grading.

## Data Loading and Exploratory Analysis


```{r include=FALSE}
library(knitr)
library(caret)
library(rpart)
library(rpart.plot)
library(randomForest)
library(corrplot)
library(rattle)
```

## Loading the data

The training data for this project are downloaded from [here][https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv]

The test data are downloaded from [here][https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv]

The data for this project come from this [source][http://groupware.les.inf.puc-rio.br/har]

```{r }
training <- read.csv("./data/pml-training.csv")
testing <- read.csv("./data/pml-testing.csv")
inTrain  <- createDataPartition(training$classe, p=0.7, list=FALSE)
TrainSet <- training[inTrain, ]
TestSet  <- training[-inTrain, ]
```

After downloading the data from the data source, we can read the two csv files into two data frames. 

```{r}
NZV <- nearZeroVar(TrainSet)
TrainSet <- TrainSet[, -NZV]
TestSet  <- TestSet[, -NZV]
dim(TrainSet); dim(TestSet)
```

The training data set contains 19622 observations and 160 variables, while the testing data set contains 20 observations and 160 variables. The "classe" variable in the training set is the outcome to predict.

## Clean The Data

In this step, we will clean the data and get rid of observations with missing values as well as some meaningless variables.

```{r}
AllNA    <- sapply(TrainSet, function(x) mean(is.na(x))) > 0.95
TrainSet <- TrainSet[, AllNA==FALSE]
TestSet  <- TestSet[, AllNA==FALSE]
dim(TrainSet)
```

First, we remove columns that contain NA missing values, i.e. are unnessary.

```{r}
TrainSet <- TrainSet[, -(1:5)]
TestSet  <- TestSet[, -(1:5)]
dim(TrainSet); dim(TestSet)
```

Verifying Correlation Analysis

```{r}
corMatrix <- cor(TrainSet[, -54])
corrplot(corMatrix, order = "FPC", method = "color", type = "lower", 
         tl.cex = 0.8, tl.col = rgb(0, 0, 0))
```

Now, the cleaned training data set contains 19622 observations and 53 variables, while the testing data set contains 20 observations and 53 variables. The "classe" variable is still in the cleaned training set.

## Slice the data

Then, we can split the cleaned training set into a pure training data set (70%) and a validation data set (30%). We will use the validation data set to conduct cross validation in future steps. 

Random Forest

```{r}
set.seed(12345)
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
modFitRandForest <- train(classe ~ ., data=TrainSet, method="rf",
                          trControl=controlRF)
modFitRandForest$finalModel
```

## Data Modeling

We fit a predictive model for activity recognition using Random Forest algorithm because it automatically selects important variables and is robust to correlated covariates & outliers in general. We will use 5-fold cross validation when applying the algorithm. 

Then, we estimate the performance of the model on the validation data set. Predict on test dataset

```{r}
predictRandForest <- predict(modFitRandForest, newdata=TestSet)
confMatRandForest <- confusionMatrix(predictRandForest, TestSet$classe)
confMatRandForest
```

```{r}
plot(confMatRandForest$table, col = "beige",
     main = paste("Random Forest - Accuracy =",
                  round(confMatRandForest$overall['Accuracy'], 4)))
```
## Decision Trees Method

```{r}
set.seed(12345)
modFitDecTree <- rpart(classe ~ ., data=TrainSet, method="class")
suppressWarnings(fancyRpartPlot(modFitDecTree))
```

# Test on Test dataset

```{r}
predictDecTree <- predict(modFitDecTree, newdata=TestSet, type="class")
confMatDecTree <- confusionMatrix(predictDecTree, TestSet$classe)
confMatDecTree
```

Ploting Matrix Results

```{r}
plot(confMatDecTree$table, col = "bisque", 
     main = paste("Decision Tree - Accuracy =",
                  round(confMatDecTree$overall['Accuracy'], 4)))
```

# General Boasted Model

```{r}
set.seed(12345)
controlGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
modFitGBM  <- train(classe ~ ., data=TrainSet, method = "gbm",
                    trControl = controlGBM, verbose = FALSE)
modFitGBM$finalModel
```

Predict on test dataset for GBM

```{r}
predictGBM <- predict(modFitGBM, newdata=TestSet)
confMatGBM <- confusionMatrix(predictGBM, TestSet$classe)
confMatGBM
```

Plot matrix result

```{r}
plot(confMatGBM$table, col = "aquamarine3", 
     main = paste("GBM - Accuracy =", round(confMatGBM$overall['Accuracy'], 4)))
```
The accuracy of the 3 regression modeling methods above are:

Random Forest : 0.9963 Decision Tree : 0.7368 GBM : 0.9839


## Cross Validationi Model

Data Preparation

```{r}
training<-read.csv("./data/pml-training.csv",na.strings=c("NA","#DIV/0!"))
testing<-read.csv("./data/pml-testing.csv",na.strings=c("NA","#DIV/0!"))
table(training$classe)
NA_Count = sapply(1:dim(training)[2],function(x)sum(is.na(training[,x])))
NA_list = which(NA_Count>0)
colnames(training[,c(1:7)])
training = training[,-NA_list]
training = training[,-c(1:7)]
training$classe = factor(training$classe)
testing = testing[,-NA_list]
testing = testing[,-c(1:7)]
```

```{r}
set.seed(1234)
cv3 = trainControl(method="cv",number=3,allowParallel=TRUE,verboseIter=TRUE)
modrf = train(classe~., data=training, method="rf",trControl=cv3)
modtree = train(classe~.,data=training,method="rpart",trControl=cv3)
```

```{r}
prf=predict(modrf,training)
ptree=predict(modtree,training)
table(prf,training$classe); table(ptree,training$classe)
```

```{r}
prf=predict(modrf,testing)
ptree=predict(modtree,testing)
table(prf,ptree)
```
Random forest model has the best accuracy for testing datas

## Conclusion

```{r}
answers=predict(modrf,testing)
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
answers
```

pml_write_files(answers)

Other conclusion is that 52 variables to build the random forest model with 3-fold cross validation. The out-of-sample error is approximately 0.9%.

The predicted classes for the 20 tests are: B A B A A E D B A A B C B A E E A B B B.





